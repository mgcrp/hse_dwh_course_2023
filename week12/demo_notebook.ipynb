{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b39fe54",
   "metadata": {},
   "source": [
    "## Разбор ДЗ\n",
    "\n",
    "1) У нас есть master и slave (week02)\n",
    "\n",
    "2) Как проверить репликацию:\n",
    "    - На master `select * from pg_stat_replication;`\n",
    "    - На replica `select * from pg_stat_wal_receiver;`\n",
    "\n",
    "3) Хотим init из файла\n",
    "```sql\n",
    "  CREATE TABLE manufacturers (\n",
    "    manufacturer_id SERIAL PRIMARY KEY,\n",
    "    manufacturer_name VARCHAR(100) NOT NULL,\n",
    "    manufacturer_legal_entity VARCHAR(100) NOT NULL\n",
    "  );\n",
    "\n",
    "  CREATE TABLE categories (\n",
    "    category_id SERIAL PRIMARY KEY,\n",
    "    category_name VARCHAR(100) NOT NULL\n",
    "  );\n",
    "\n",
    "  CREATE TABLE products (\n",
    "    category_id BIGINT NOT NULL,\n",
    "    manufacturer_id BIGINT NOT NULL,\n",
    "    product_id SERIAL PRIMARY KEY,\n",
    "    product_name VARCHAR(255) NOT NULL,\n",
    "    product_picture_url VARCHAR(255) NOT NULL,\n",
    "    product_description VARCHAR(255) NOT NULL,\n",
    "    product_age_restriction INT NOT NULL,\n",
    "    CONSTRAINT category_fk FOREIGN KEY (category_id) REFERENCES categories (category_id),\n",
    "    CONSTRAINT manufacturer_fk FOREIGN KEY (manufacturer_id) REFERENCES manufacturers (manufacturer_id)\n",
    "  );\n",
    "\n",
    "  CREATE TABLE stores (\n",
    "    store_id SERIAL PRIMARY KEY,\n",
    "    store_name VARCHAR(100) NOT NULL,\n",
    "    store_country VARCHAR(255) NOT NULL,\n",
    "    store_city VARCHAR(255) NOT NULL,\n",
    "    store_address VARCHAR(255) NOT NULL\n",
    "  );\n",
    "\n",
    "  CREATE TABLE customers (\n",
    "    customer_id SERIAL PRIMARY KEY,\n",
    "    customer_fname VARCHAR(100) NOT NULL,\n",
    "    customer_lname VARCHAR(100) NOT NULL,\n",
    "    customer_gender VARCHAR(100) NOT NULL,\n",
    "    customer_phone VARCHAR(100) NOT NULL\n",
    "  );\n",
    "\n",
    "  CREATE TABLE price_change (\n",
    "    product_id BIGINT NOT NULL,\n",
    "    price_change_ts TIMESTAMP NOT NULL,\n",
    "    new_price BIGINT NOT NULL,\n",
    "    CONSTRAINT product_fk FOREIGN KEY (product_id) REFERENCES products (product_id),\n",
    "    PRIMARY KEY (product_id, price_change_ts)\n",
    "  );\n",
    "\n",
    "  CREATE TABLE deliveries (\n",
    "    delivery_id BIGINT PRIMARY KEY,\n",
    "    store_id BIGINT NOT NULL,\n",
    "    product_id BIGINT NOT NULL,\n",
    "    delivery_date DATE NOT NULL,\n",
    "    product_count INTEGER NOT NULL,\n",
    "    CONSTRAINT store_fk FOREIGN KEY (store_id) REFERENCES stores (store_id),\n",
    "    CONSTRAINT product_fk FOREIGN KEY (product_id) REFERENCES products (product_id)\n",
    "  );\n",
    "\n",
    "  CREATE TABLE purchases (\n",
    "    store_id BIGINT NOT NULL,\n",
    "    customer_id BIGINT NOT NULL,\n",
    "    purchase_id SERIAL PRIMARY KEY,\n",
    "    purchase_date TIMESTAMP NOT NULL,\n",
    "    purchase_payment_type VARCHAR(100) NOT NULL,\n",
    "    CONSTRAINT store_fk FOREIGN KEY (store_id) REFERENCES stores (store_id),\n",
    "    CONSTRAINT customer_fk FOREIGN KEY (customer_id) REFERENCES customers (customer_id)\n",
    "  );\n",
    "\n",
    "  CREATE TABLE purchase_items (\n",
    "    product_id BIGINT NOT NULL,\n",
    "    purchase_id BIGINT NOT NULL,\n",
    "    product_count BIGINT NOT NULL,\n",
    "    product_price BIGINT NOT NULL,\n",
    "    CONSTRAINT product_fk FOREIGN KEY (product_id) REFERENCES products (product_id),\n",
    "    CONSTRAINT purchase_fk FOREIGN KEY (purchase_id) REFERENCES purchases (purchase_id),\n",
    "    PRIMARY KEY (product_id, purchase_id)\n",
    "  );\n",
    "```\n",
    "<br>\n",
    "Для этого в docker-compose\n",
    "\n",
    "```yml\n",
    "  volumes:\n",
    "    - ./db-init.sql:/docker-entrypoint-initdb.d/db-init.sql\n",
    "```\n",
    "\n",
    "4)\n",
    "```sql\n",
    "  CREATE VIEW gmv as (\n",
    "    SELECT\n",
    "      pu.store_id,\n",
    "      pr.category_id,\n",
    "      SUM(pui.product_count * pui.product_price) AS sales_sum\n",
    "    FROM purchases pu\n",
    "    JOIN purchase_items pui\n",
    "      ON pu.purchase_id = pui.purchase_id\n",
    "    JOIN products pr\n",
    "      ON pui.product_id = pr.product_id\n",
    "    GROUP BY\n",
    "      pu.store_id,\n",
    "      pr.category_id\n",
    "  );\n",
    "```\n",
    "<br>\n",
    "\n",
    "Для этого в docker-compose\n",
    "```yml\n",
    "  volumes:\n",
    "    - ./db-init.sql:/docker-entrypoint-initdb.d/db-init.sql\n",
    "    - ./view-init.sql:/docker-entrypoint-initdb.d/view-init.sql\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc071a7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8da228",
   "metadata": {},
   "source": [
    "5) Поднять debezium (week05)\n",
    "\n",
    "```yml\n",
    "version: '3'\n",
    "\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:7.3.1\n",
    "    hostname: zookeeper\n",
    "    container_name: zookeeper\n",
    "    ports:\n",
    "      - '2181:2181'\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    healthcheck:\n",
    "      test: echo srvr | nc zookeeper 2181 || exit 1\n",
    "      start_period: 10s\n",
    "      retries: 20\n",
    "      interval: 10s\n",
    "\n",
    "  broker:\n",
    "    image: confluentinc/cp-kafka:7.3.1\n",
    "    hostname: broker\n",
    "    container_name: broker\n",
    "    depends_on:\n",
    "      zookeeper:\n",
    "        condition: service_healthy\n",
    "    ports:\n",
    "      - '9092:9092'\n",
    "      - '9101:9101'\n",
    "      - '29092:29092'\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n",
    "      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n",
    "      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n",
    "      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'\n",
    "      KAFKA_JMX_PORT: 9101\n",
    "      KAFKA_JMX_HOSTNAME: localhost\n",
    "    healthcheck:\n",
    "      test: nc -z localhost 9092 || exit -1\n",
    "      start_period: 15s\n",
    "      interval: 5s\n",
    "      timeout: 10s\n",
    "      retries: 10\n",
    "      \n",
    "  rest-proxy:\n",
    "    image: confluentinc/cp-kafka-rest:7.3.1\n",
    "    hostname: rest-proxy\n",
    "    container_name: rest-proxy\n",
    "    depends_on:\n",
    "      broker:\n",
    "        condition: service_healthy\n",
    "    ports:\n",
    "      - '8082:8082'\n",
    "    environment:\n",
    "      KAFKA_REST_HOST_NAME: rest-proxy\n",
    "      KAFKA_REST_BOOTSTRAP_SERVERS: 'broker:29092'\n",
    "      KAFKA_REST_LISTENERS: 'http://0.0.0.0:8082'\n",
    "\n",
    "  debezium:\n",
    "    image: debezium/connect:latest\n",
    "    container_name: debezium\n",
    "    hostname: debezium\n",
    "    depends_on:\n",
    "      broker:\n",
    "        condition: service_healthy\n",
    "    restart: always\n",
    "    ports:\n",
    "      - '8083:8083'\n",
    "    environment:\n",
    "      BOOTSTRAP_SERVERS: broker:29092\n",
    "      GROUP_ID: 1\n",
    "      CONFIG_STORAGE_TOPIC: connect_configs\n",
    "      STATUS_STORAGE_TOPIC: connect_statuses\n",
    "      OFFSET_STORAGE_TOPIC: connect_offsets\n",
    "      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter\n",
    "      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter\n",
    "      ENABLE_DEBEZIUM_SCRIPTING: 'true'\n",
    "    healthcheck:\n",
    "      test:\n",
    "        [\n",
    "          'CMD',\n",
    "          'curl',\n",
    "          '--silent',\n",
    "          '--fail',\n",
    "          '-X',\n",
    "          'GET',\n",
    "          'http://localhost:8083/connectors',\n",
    "        ]\n",
    "      start_period: 10s\n",
    "      interval: 10s\n",
    "      timeout: 5s\n",
    "      retries: 5\n",
    "\n",
    "  debezium-ui:\n",
    "    image: debezium/debezium-ui:latest\n",
    "    container_name: debezium-ui\n",
    "    hostname: debezium-ui\n",
    "    depends_on:\n",
    "      debezium:\n",
    "        condition: service_healthy\n",
    "    restart: always\n",
    "    ports:\n",
    "      - '8080:8080'\n",
    "    environment:\n",
    "      KAFKA_CONNECT_URIS: http://debezium:8083\n",
    "```\n",
    "\n",
    "6) Создадим connection\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"pg-connector\",\n",
    "  \"config\": {\n",
    "    \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n",
    "    \"plugin.name\": \"pgoutput\",\n",
    "    \n",
    "    \"database.hostname\": \"host.docker.internal\",\n",
    "    \"database.port\": \"5432\",\n",
    "    \"database.user\": \"postgres\",\n",
    "    \"database.password\": \"postgres\",\n",
    "    \"database.dbname\" : \"postgres\",\n",
    "    \"database.server.name\": \"pg-dev\",\n",
    "    \n",
    "    \"table.include.list\": \"public.(.*)\",\n",
    "    \"heartbeat.interval.ms\": \"5000\",\n",
    "    \"slot.name\": \"dbname_debezium\",\n",
    "    \"publication.name\": \"dbname_publication\",\n",
    "      \n",
    "    \"topic.creation.default.cleanup.policy\": \"delete\",\n",
    "    \"topic.creation.default.partitions\": \"1\",\n",
    "    \"topic.creation.default.replication.factor\": \"1\",\n",
    "    \"topic.creation.default.retention.ms\": \"604800000\",\n",
    "    \"topic.creation.enable\": \"true\",\n",
    "    \"topic.prefix\": \"postgres\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25a73137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201,\n",
       " '{\"name\":\"pg-connector\",\"config\":{\"connector.class\":\"io.debezium.connector.postgresql.PostgresConnector\",\"plugin.name\":\"pgoutput\",\"database.hostname\":\"host.docker.internal\",\"database.port\":\"5432\",\"database.user\":\"postgres\",\"database.password\":\"postgres\",\"database.dbname\":\"postgres\",\"database.server.name\":\"pg-dev\",\"table.include.list\":\"public.(.*)\",\"heartbeat.interval.ms\":\"5000\",\"slot.name\":\"dbname_debezium\",\"publication.name\":\"dbname_publication\",\"topic.creation.default.cleanup.policy\":\"delete\",\"topic.creation.default.partitions\":\"1\",\"topic.creation.default.replication.factor\":\"1\",\"topic.creation.default.retention.ms\":\"604800000\",\"topic.creation.enable\":\"true\",\"topic.prefix\":\"postgres\",\"name\":\"pg-connector\"},\"tasks\":[],\"type\":\"source\"}')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as rq\n",
    "\n",
    "response = rq.post(\n",
    "    url='http://localhost:8083/connectors',\n",
    "    json={\n",
    "        \"name\": \"pg-connector\",\n",
    "        \"config\": {\n",
    "        \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n",
    "        \"plugin.name\": \"pgoutput\",\n",
    "\n",
    "        \"database.hostname\": \"host.docker.internal\",\n",
    "        \"database.port\": \"5432\",\n",
    "        \"database.user\": \"postgres\",\n",
    "        \"database.password\": \"postgres\",\n",
    "        \"database.dbname\" : \"postgres\",\n",
    "        \"database.server.name\": \"pg-dev\",\n",
    "\n",
    "        \"table.include.list\": \"public.(.*)\",\n",
    "        \"heartbeat.interval.ms\": \"5000\",\n",
    "        \"slot.name\": \"dbname_debezium\",\n",
    "        \"publication.name\": \"dbname_publication\",\n",
    "\n",
    "        \"topic.creation.default.cleanup.policy\": \"delete\",\n",
    "        \"topic.creation.default.partitions\": \"1\",\n",
    "        \"topic.creation.default.replication.factor\": \"1\",\n",
    "        \"topic.creation.default.retention.ms\": \"604800000\",\n",
    "        \"topic.creation.enable\": \"true\",\n",
    "        \"topic.prefix\": \"postgres\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "response.status_code, response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03712647",
   "metadata": {},
   "source": [
    "7) Смотрим, что все создалось http://localhost:8080\n",
    "\n",
    "8) Создадим postgres_dwh\n",
    "\n",
    "```yml\n",
    "  postgres_dwh:\n",
    "    container_name: postgres_dwh\n",
    "    image: postgres:14.5\n",
    "    restart: always\n",
    "    ports:\n",
    "      - \"5434:5432\"\n",
    "    environment:\n",
    "      - POSTGRES_USER=postgres\n",
    "      - POSTGRES_PASSWORD=postgres\n",
    "```\n",
    "\n",
    "9) Пульнем тестовые данные\n",
    "\n",
    "```sql\n",
    "INSERT INTO manufacturers (manufacturer_name, manufacturer_legal_entity) VALUES\n",
    "('Manufacturer A', 'OOO A'), ('Manufacturer B', 'OAO B'), ('Manufacturer C', 'IP C');\n",
    "\n",
    "INSERT INTO categories (category_name) VALUES\n",
    "('Electronics'), ('Clothing'), ('Groceries');\n",
    "\n",
    "INSERT INTO products (\n",
    "    product_id, product_name, product_picture_url, product_description, product_age_restriction,\n",
    "    category_id, manufacturer_id\n",
    ") VALUES\n",
    "(1, 'Smartphone', 'data:image/jpeg;base64,/9j/4AAQSk', 'lol', 12, 1, 1),\n",
    "(2, 'T-Shirt', 'data:image/jpeg;base64,/9j/IJHDSUD', 'kek', 14, 2, 2),\n",
    "(3, 'Bread', 'data:image/jpeg;base64,/9j/I6SSYA', 'puk', 0, 3, 3);\n",
    "\n",
    "INSERT INTO stores (store_name, store_country, store_city, store_address) VALUES\n",
    "('Store X', 'Russia', 'Moscow', '1 Chapel Hill'),\n",
    "('Store Y', 'Belarus', 'Minsk', '438 DARK SPURT'),\n",
    "('Store Z', 'Ukraine', 'Kiev', 'ul. Kosmonavtov 35-11');\n",
    "\n",
    "INSERT INTO price_change (product_id, price_change_ts, new_price) VALUES\n",
    "(1, '2023-10-15 12:00:00', 520.00), (2, '2023-10-15 13:00:00', 14.00);\n",
    "\n",
    "INSERT INTO deliveries (delivery_id, store_id, product_id, delivery_date, product_count) VALUES\n",
    "(1, 1, 1, '2023-10-15', 100), (2, 2, 2, '2023-10-14', 200);\n",
    "\n",
    "INSERT INTO customers (customer_fname, customer_lname, customer_gender, customer_phone) VALUES\n",
    "('John', 'Doe', 'man', '88005553535'),\n",
    "('Jane', 'Smith', 'woman', '88006664548'),\n",
    "('Alice', 'Johnson', 'woman', '89155673451');\n",
    "\n",
    "INSERT INTO purchases (store_id, customer_id, purchase_date, purchase_payment_type) VALUES\n",
    "(1, 1, '2023-10-15', 'cash'), (2, 2, '2023-10-15', 'card'), (3, 3, '2023-10-14', 'cash');\n",
    "\n",
    "INSERT INTO purchase_items (product_id, purchase_id, product_count, product_price) VALUES\n",
    "(1, 1, 2, 500.00), (2, 2, 3, 15.00), (3, 3, 5, 2.00);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "575b01d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"kind\":\"KafkaClusterList\",\"metadata\":{\"self\":\"http://rest-proxy:8082/v3/clusters\",\"next\":null},\"data\":[{\"kind\":\"KafkaCluster\",\"metadata\":{\"self\":\"http://rest-proxy:8082/v3/clusters/zjy3h7GlTPCAIcDuLZ2cGg\",\"resource_name\":\"crn:///kafka=zjy3h7GlTPCAIcDuLZ2cGg\"},\"cluster_id\":\"zjy3h7GlTPCAIcDuLZ2cGg\",\"controller\":{\"related\":\"http://rest-proxy:8082/v3/clusters/zjy3h7GlTPCAIcDuLZ2cGg/brokers/1\"},\"acls\":{\"related\":\"http://rest-proxy:8082/v3/clusters/zjy3h7GlTPCAIcDuLZ2cGg/acls\"},\"brokers\":{\"related\":\"http://rest-proxy:8082/v3/clusters/zjy3h7GlTPCAIcDuLZ2cGg/brokers\"},\"broker_configs\":{\"related\":\"http://rest-proxy:8082/v3/clusters/zjy3h7GlTPCAIcDuLZ2cGg/broker-configs\"},\"consumer_groups\":{\"related\":\"http://rest-proxy:8082/v3/clusters/zjy3h7GlTPCAIcDuLZ2cGg/consumer-groups\"},\"topics\":{\"related\":\"http://rest-proxy:8082/v3/clusters/zjy3h7GlTPCAIcDuLZ2cGg/topics\"},\"partition_reassignments\":{\"related\":\"http://rest-proxy:8082/v3/clusters/zjy3h7GlTPCAIcDuLZ2cGg/topics/-/partitions/-/reassignment\"}}]}"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:8082/v3/clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f49ad761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__debezium-heartbeat.postgres\n",
      "connect_configs\n",
      "connect_offsets\n",
      "connect_statuses\n",
      "postgres.public.categories\n",
      "postgres.public.customers\n",
      "postgres.public.deliveries\n",
      "postgres.public.manufacturers\n",
      "postgres.public.price_change\n",
      "postgres.public.products\n",
      "postgres.public.purchase_items\n",
      "postgres.public.purchases\n",
      "postgres.public.stores\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('topics.json', 'r') as f:\n",
    "    topics = json.load(f)\n",
    "    for i in topics['data']:\n",
    "        print(i['topic_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1936e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "kafka_server = 'localhost:9092'\n",
    "kafka_topics = [\n",
    "    'postgres.public.categories',\n",
    "    'postgres.public.customers',\n",
    "    'postgres.public.deliveries',\n",
    "    'postgres.public.manufacturers',\n",
    "    'postgres.public.price_change',\n",
    "    'postgres.public.products',\n",
    "    'postgres.public.purchase_items',\n",
    "    'postgres.public.purchases',\n",
    "    'postgres.public.stores',\n",
    "]\n",
    "kafka_consumer_group = 'backend'\n",
    "dwh_conn_string = 'postgresql+psycopg2://postgres:postgres@localhost:5434/postgres'\n",
    "\n",
    "def read_single_message(topic=None):\n",
    "    assert topic is not None, 'You must specify topic name'\n",
    "    \n",
    "    consumer = KafkaConsumer(\n",
    "        bootstrap_servers=kafka_server,\n",
    "        value_deserializer=lambda v: v if v is None else json.loads( v.decode(\"utf-8\") ),\n",
    "        auto_offset_reset=\"earliest\",\n",
    "        group_id=kafka_consumer_group\n",
    "    )\n",
    "    consumer.subscribe(topics=topic)\n",
    "\n",
    "    try:\n",
    "        for message in consumer:\n",
    "            value = message.value\n",
    "            return value\n",
    "    except Exception as e:\n",
    "            print(\"Closing consumer due to error\\n\")\n",
    "            consumer.close()\n",
    "            raise e\n",
    "    finally:\n",
    "        print(\"Closing consumer due to finish\\n\")\n",
    "        consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43225573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing consumer due to finish\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'schema': {'type': 'struct',\n",
       "  'fields': [{'type': 'struct',\n",
       "    'fields': [{'type': 'int64', 'optional': False, 'field': 'store_id'},\n",
       "     {'type': 'int64', 'optional': False, 'field': 'customer_id'},\n",
       "     {'type': 'int32',\n",
       "      'optional': False,\n",
       "      'default': 0,\n",
       "      'field': 'purchase_id'},\n",
       "     {'type': 'int64',\n",
       "      'optional': False,\n",
       "      'name': 'io.debezium.time.MicroTimestamp',\n",
       "      'version': 1,\n",
       "      'field': 'purchase_date'},\n",
       "     {'type': 'string', 'optional': False, 'field': 'purchase_payment_type'}],\n",
       "    'optional': True,\n",
       "    'name': 'postgres.public.purchases.Value',\n",
       "    'field': 'before'},\n",
       "   {'type': 'struct',\n",
       "    'fields': [{'type': 'int64', 'optional': False, 'field': 'store_id'},\n",
       "     {'type': 'int64', 'optional': False, 'field': 'customer_id'},\n",
       "     {'type': 'int32',\n",
       "      'optional': False,\n",
       "      'default': 0,\n",
       "      'field': 'purchase_id'},\n",
       "     {'type': 'int64',\n",
       "      'optional': False,\n",
       "      'name': 'io.debezium.time.MicroTimestamp',\n",
       "      'version': 1,\n",
       "      'field': 'purchase_date'},\n",
       "     {'type': 'string', 'optional': False, 'field': 'purchase_payment_type'}],\n",
       "    'optional': True,\n",
       "    'name': 'postgres.public.purchases.Value',\n",
       "    'field': 'after'},\n",
       "   {'type': 'struct',\n",
       "    'fields': [{'type': 'string', 'optional': False, 'field': 'version'},\n",
       "     {'type': 'string', 'optional': False, 'field': 'connector'},\n",
       "     {'type': 'string', 'optional': False, 'field': 'name'},\n",
       "     {'type': 'int64', 'optional': False, 'field': 'ts_ms'},\n",
       "     {'type': 'string',\n",
       "      'optional': True,\n",
       "      'name': 'io.debezium.data.Enum',\n",
       "      'version': 1,\n",
       "      'parameters': {'allowed': 'true,last,false,incremental'},\n",
       "      'default': 'false',\n",
       "      'field': 'snapshot'},\n",
       "     {'type': 'string', 'optional': False, 'field': 'db'},\n",
       "     {'type': 'string', 'optional': True, 'field': 'sequence'},\n",
       "     {'type': 'string', 'optional': False, 'field': 'schema'},\n",
       "     {'type': 'string', 'optional': False, 'field': 'table'},\n",
       "     {'type': 'int64', 'optional': True, 'field': 'txId'},\n",
       "     {'type': 'int64', 'optional': True, 'field': 'lsn'},\n",
       "     {'type': 'int64', 'optional': True, 'field': 'xmin'}],\n",
       "    'optional': False,\n",
       "    'name': 'io.debezium.connector.postgresql.Source',\n",
       "    'field': 'source'},\n",
       "   {'type': 'string', 'optional': False, 'field': 'op'},\n",
       "   {'type': 'int64', 'optional': True, 'field': 'ts_ms'},\n",
       "   {'type': 'struct',\n",
       "    'fields': [{'type': 'string', 'optional': False, 'field': 'id'},\n",
       "     {'type': 'int64', 'optional': False, 'field': 'total_order'},\n",
       "     {'type': 'int64', 'optional': False, 'field': 'data_collection_order'}],\n",
       "    'optional': True,\n",
       "    'name': 'event.block',\n",
       "    'version': 1,\n",
       "    'field': 'transaction'}],\n",
       "  'optional': False,\n",
       "  'name': 'postgres.public.purchases.Envelope',\n",
       "  'version': 1},\n",
       " 'payload': {'before': None,\n",
       "  'after': {'store_id': 1,\n",
       "   'customer_id': 1,\n",
       "   'purchase_id': 1,\n",
       "   'purchase_date': 1697328000000000,\n",
       "   'purchase_payment_type': 'cash'},\n",
       "  'source': {'version': '2.2.0.Alpha3',\n",
       "   'connector': 'postgresql',\n",
       "   'name': 'postgres',\n",
       "   'ts_ms': 1702396316890,\n",
       "   'snapshot': 'false',\n",
       "   'db': 'postgres',\n",
       "   'sequence': '[\"50856440\",\"50856544\"]',\n",
       "   'schema': 'public',\n",
       "   'table': 'purchases',\n",
       "   'txId': 758,\n",
       "   'lsn': 50856544,\n",
       "   'xmin': None},\n",
       "  'op': 'c',\n",
       "  'ts_ms': 1702396317133,\n",
       "  'transaction': None}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = read_single_message('postgres.public.purchases')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dc0511b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'before': None,\n",
       " 'after': {'store_id': 1,\n",
       "  'customer_id': 1,\n",
       "  'purchase_id': 1,\n",
       "  'purchase_date': 1697328000000000,\n",
       "  'purchase_payment_type': 'cash'},\n",
       " 'source': {'version': '2.2.0.Alpha3',\n",
       "  'connector': 'postgresql',\n",
       "  'name': 'postgres',\n",
       "  'ts_ms': 1702396316890,\n",
       "  'snapshot': 'false',\n",
       "  'db': 'postgres',\n",
       "  'sequence': '[\"50856440\",\"50856544\"]',\n",
       "  'schema': 'public',\n",
       "  'table': 'purchases',\n",
       "  'txId': 758,\n",
       "  'lsn': 50856544,\n",
       "  'xmin': None},\n",
       " 'op': 'c',\n",
       " 'ts_ms': 1702396317133,\n",
       " 'transaction': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['payload']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb72e2",
   "metadata": {},
   "source": [
    "10) Посмотрим на stores и придуем, что с ним делать\n",
    "\n",
    "```sql\n",
    "CREATE TABLE stores (\n",
    "    store_id SERIAL PRIMARY KEY,\n",
    "    store_name VARCHAR(100) NOT NULL,\n",
    "    store_country VARCHAR(255) NOT NULL,\n",
    "    store_city VARCHAR(255) NOT NULL,\n",
    "    store_address VARCHAR(255) NOT NULL\n",
    "  );\n",
    "```\n",
    "\n",
    "```sql\n",
    "create schema if not exists dwh_detailed;\n",
    "\n",
    "create table if not exists dwh_detailed.anc_sourceSystem (\n",
    "    sourceSystemID    VARCHAR(128) PRIMARY KEY,\n",
    "    created_at        TIMESTAMP NOT NULL DEFAULT NOW()\n",
    ");\n",
    "\n",
    "create table if not exists dwh_detailed.attr_sourceSystem_name (\n",
    "    sourceSystemID    VARCHAR(128) UNIQUE NOT NULL,\n",
    "    name              VARCHAR(255) NOT NULL,\n",
    "    created_at        TIMESTAMP NOT NULL DEFAULT NOW()\n",
    ");\n",
    "\n",
    "create table if not exists dwh_detailed.anc_store (\n",
    "    storeId           VARCHAR(128) PRIMARY KEY,\n",
    "    sourceSystemID    VARCHAR(128) NOT NULL,\n",
    "    created_at        TIMESTAMP NOT NULL DEFAULT NOW()\n",
    ");\n",
    "\n",
    "create table if not exists dwh_detailed.attr_store_businessKey (\n",
    "    storeId           VARCHAR(128) UNIQUE NOT NULL,\n",
    "    businessKey       BIGINT NOT NULL,\n",
    "    created_at        TIMESTAMP NOT NULL DEFAULT NOW()\n",
    ");\n",
    "\n",
    "create table if not exists dwh_detailed.attr_store_isActive (\n",
    "    storeId           VARCHAR(128) UNIQUE NOT NULL,\n",
    "    isActive          BOOL NOT NULL,\n",
    "    created_at        TIMESTAMP NOT NULL DEFAULT NOW()\n",
    ");\n",
    "\n",
    "create table if not exists dwh_detailed.attr_store_name (\n",
    "    storeId           VARCHAR(128) UNIQUE NOT NULL,\n",
    "    storeName         VARCHAR(255) NOT NULL,\n",
    "    created_at        TIMESTAMP NOT NULL DEFAULT NOW()\n",
    ");\n",
    "\n",
    "create table if not exists dwh_detailed.attr_store_address (\n",
    "    storeId           VARCHAR(128) UNIQUE NOT NULL,\n",
    "    storeAddress      VARCHAR(255) NOT NULL,\n",
    "    created_at        TIMESTAMP NOT NULL DEFAULT NOW()\n",
    ");\n",
    "\n",
    "create table if not exists dwh_detailed.attr_store_city (\n",
    "    storeId           VARCHAR(128) UNIQUE NOT NULL,\n",
    "    storeCity         VARCHAR(255) NOT NULL,\n",
    "    created_at        TIMESTAMP NOT NULL DEFAULT NOW()\n",
    ");\n",
    "\n",
    "create table if not exists dwh_detailed.anc_country (\n",
    "    countryId         VARCHAR(128) PRIMARY KEY,\n",
    "    sourceSystemID    VARCHAR(128) NOT NULL,\n",
    "    created_at        TIMESTAMP NOT NULL DEFAULT NOW()\n",
    ");\n",
    "\n",
    "create table if not exists dwh_detailed.attr_country_name (\n",
    "    countryId         VARCHAR(128) UNIQUE NOT NULL,\n",
    "    countryName       VARCHAR(255) NOT NULL,\n",
    "    created_at        TIMESTAMP NOT NULL DEFAULT NOW()\n",
    ");\n",
    "\n",
    "create table if not exists dwh_detailed.tie_store_country (\n",
    "    storeId           VARCHAR(128) NOT NULL,\n",
    "    countryId         VARCHAR(128) NOT NULL,\n",
    "    created_at        TIMESTAMP NOT NULL DEFAULT NOW()\n",
    ");\n",
    "\n",
    "ALTER TABLE dwh_detailed.tie_store_country\n",
    "ADD CONSTRAINT uq_tie_store_country UNIQUE(storeId, countryId);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cee4932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "class StoreHandler:\n",
    "    dwh_engine = None\n",
    "    kafka_engine = None\n",
    "    \n",
    "    def __init__(self, dwh_conn_string, kafka_host, kafka_consumer_group, kafka_topic):\n",
    "        self.dwh_engine = create_engine(dwh_conn_string)\n",
    "        self.kafka_engine = KafkaConsumer(\n",
    "            bootstrap_servers=kafka_host,\n",
    "            value_deserializer=lambda v: v if v is None else json.loads( v.decode(\"utf-8\") ),\n",
    "            auto_offset_reset=\"earliest\",\n",
    "            group_id=kafka_consumer_group\n",
    "        )\n",
    "        self.kafka_engine.subscribe(topics=kafka_topic)\n",
    "        \n",
    "    def compute_hash(self, obj):\n",
    "        return hashlib.md5(obj.encode()).hexdigest()\n",
    "    \n",
    "    def process_row(self, row):\n",
    "        if (test['payload']['before'] is None) and (test['payload']['after'] is not None):\n",
    "            # create query\n",
    "            query = ''\n",
    "            # 1 - source system\n",
    "            source_system_name = row['payload']['source']['name']\n",
    "            source_system_hash = self.compute_hash(source_system_name)\n",
    "            query += f\"insert into dwh_detailed.anc_sourceSystem (sourceSystemID) values ('{source_system_hash}') on conflict (sourceSystemID) do nothing;\\n\"\n",
    "            query += f\"insert into dwh_detailed.attr_sourceSystem_name (sourceSystemID, name) values ('{source_system_hash}', '{source_system_name}') on conflict (sourceSystemID) do nothing;\\n\"\n",
    "            # 2 - store\n",
    "            store_id = row['payload']['after']['store_id']\n",
    "            store_hash = self.compute_hash( str(store_id) )\n",
    "            store_name = row['payload']['after']['store_name']\n",
    "            store_address = row['payload']['after']['store_address']\n",
    "            store_city = row['payload']['after']['store_city']\n",
    "            query += f\"insert into dwh_detailed.anc_store (storeId, sourceSystemID) values ('{store_hash}', '{source_system_hash}') on conflict (storeId) do nothing;\\n\"\n",
    "            query += f\"insert into dwh_detailed.attr_store_businessKey (storeId, businessKey) values ('{store_hash}', '{store_id}') on conflict (storeId) do nothing;\\n\"\n",
    "            query += f\"insert into dwh_detailed.attr_store_name (storeId, storeName) values ('{store_hash}', '{store_name}') on conflict (storeId) do nothing;\\n\"\n",
    "            query += f\"insert into dwh_detailed.attr_store_address (storeId, storeAddress) values ('{store_hash}', '{store_address}') on conflict (storeId) do nothing;\\n\"\n",
    "            query += f\"insert into dwh_detailed.attr_store_city (storeId, storeCity) values ('{store_hash}', '{store_city}') on conflict (storeId) do nothing;\\n\"\n",
    "            query += f\"insert into dwh_detailed.attr_store_isActive (storeId, isActive) values ('{store_hash}', true) on conflict (storeId) do nothing;\\n\"\n",
    "            # 3 - country\n",
    "            country_name = row['payload']['after']['store_country']\n",
    "            country_hash = self.compute_hash(country_name)\n",
    "            query += f\"insert into dwh_detailed.anc_country (countryId, sourceSystemID) values ('{country_hash}', '{source_system_hash}') on conflict (countryId) do nothing;\\n\"\n",
    "            query += f\"insert into dwh_detailed.attr_country_name (countryId, countryName) values ('{country_hash}', '{country_name}') on conflict (countryId) do nothing;\\n\"\n",
    "            query += f\"insert into dwh_detailed.tie_store_country (storeId, countryId) values ('{store_hash}', '{country_hash}') on conflict (storeId, countryId) do nothing;\\n\"\n",
    "        elif (test['payload']['before'] is not None) and (test['payload']['after'] is None):\n",
    "            # delete query\n",
    "            query = ''\n",
    "            store_id = row['payload']['before']['store_id']\n",
    "            store_hash = self.compute_hash( str(store_id) )\n",
    "            query += f\"insert into dwh_detailed.attr_store_isActive (storeId, isActive) values ('{store_hash}', false) on conflict (storeId) do update set isActive = EXCLUDED.isActive;\\n\"\n",
    "        else:\n",
    "            # update query\n",
    "            query = ''\n",
    "            # 1 - source system\n",
    "            source_system_name = row['payload']['source']['name']\n",
    "            source_system_hash = self.compute_hash(source_system_name)\n",
    "            query += f\"insert into dwh_detailed.anc_sourceSystem (sourceSystemID) values ('{source_system_hash}') on conflict (sourceSystemID) do nothing;\\n\"\n",
    "            query += f\"insert into dwh_detailed.attr_sourceSystem_name (sourceSystemID, name) values ('{source_system_hash}', '{source_system_name}') on conflict (sourceSystemID) do nothing;\\n\"\n",
    "            # 2 - store\n",
    "            store_id = row['payload']['after']['store_id']\n",
    "            store_hash = self.compute_hash( str(store_id) )\n",
    "            store_name = row['payload']['after']['store_name']\n",
    "            store_address = row['payload']['after']['store_address']\n",
    "            store_city = row['payload']['after']['store_city']\n",
    "            query += f\"insert into dwh_detailed.attr_store_name (storeId, storeName) values ('{store_hash}', '{store_name}') on conflict (storeId) do update set storeName = EXCLUDED.storeName;\\n\"\n",
    "            query += f\"insert into dwh_detailed.attr_store_address (storeId, storeAddress) values ('{store_hash}', '{store_address}') on conflict (storeId) do update set storeAddress = EXCLUDED.storeAddress;\\n\"\n",
    "            query += f\"insert into dwh_detailed.attr_store_city (storeId, storeCity) values ('{store_hash}', '{store_city}') on conflict (storeId) do update set storeCity = EXCLUDED.storeCity;\\n\"\n",
    "            # 3 - country\n",
    "            country_name = row['payload']['after']['store_country']\n",
    "            country_hash = self.compute_hash(country_name)\n",
    "            query += f\"insert into dwh_detailed.anc_country (countryId, sourceSystemID) values ('{country_hash}', '{source_system_hash}') on conflict (countryId) do nothing;\\n\"\n",
    "            query += f\"insert into dwh_detailed.attr_country_name (countryId, countryName) values ('{country_hash}', '{country_name}') on conflict (countryId) do nothing;\\n\"\n",
    "            query += f\"insert into dwh_detailed.tie_store_country (storeId, countryId) values ('{store_hash}', '{country_hash}') on conflict (storeId, countryId) do nothing;\\n\"\n",
    "        \n",
    "        print(query)\n",
    "        with self.dwh_engine.connect() as con:\n",
    "            res = con.execute(text(query))\n",
    "            con.commit()\n",
    "        print('---')\n",
    "        \n",
    "    def consume(self):\n",
    "        try:\n",
    "            for message in self.kafka_engine:\n",
    "                print(message.value)\n",
    "                print('+++')\n",
    "                self.process_row(message.value)\n",
    "        except Exception as e:\n",
    "                print(\"Closing consumer due to error\\n\")\n",
    "                self.kafka_engine.close()\n",
    "                raise e\n",
    "        finally:\n",
    "            print(\"Closing consumer due to finish\\n\")\n",
    "            self.kafka_engine.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d99f8d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'schema': {'type': 'struct', 'fields': [{'type': 'struct', 'fields': [{'type': 'int32', 'optional': False, 'default': 0, 'field': 'store_id'}, {'type': 'string', 'optional': False, 'field': 'store_name'}, {'type': 'string', 'optional': False, 'field': 'store_country'}, {'type': 'string', 'optional': False, 'field': 'store_city'}, {'type': 'string', 'optional': False, 'field': 'store_address'}], 'optional': True, 'name': 'postgres.public.stores.Value', 'field': 'before'}, {'type': 'struct', 'fields': [{'type': 'int32', 'optional': False, 'default': 0, 'field': 'store_id'}, {'type': 'string', 'optional': False, 'field': 'store_name'}, {'type': 'string', 'optional': False, 'field': 'store_country'}, {'type': 'string', 'optional': False, 'field': 'store_city'}, {'type': 'string', 'optional': False, 'field': 'store_address'}], 'optional': True, 'name': 'postgres.public.stores.Value', 'field': 'after'}, {'type': 'struct', 'fields': [{'type': 'string', 'optional': False, 'field': 'version'}, {'type': 'string', 'optional': False, 'field': 'connector'}, {'type': 'string', 'optional': False, 'field': 'name'}, {'type': 'int64', 'optional': False, 'field': 'ts_ms'}, {'type': 'string', 'optional': True, 'name': 'io.debezium.data.Enum', 'version': 1, 'parameters': {'allowed': 'true,last,false,incremental'}, 'default': 'false', 'field': 'snapshot'}, {'type': 'string', 'optional': False, 'field': 'db'}, {'type': 'string', 'optional': True, 'field': 'sequence'}, {'type': 'string', 'optional': False, 'field': 'schema'}, {'type': 'string', 'optional': False, 'field': 'table'}, {'type': 'int64', 'optional': True, 'field': 'txId'}, {'type': 'int64', 'optional': True, 'field': 'lsn'}, {'type': 'int64', 'optional': True, 'field': 'xmin'}], 'optional': False, 'name': 'io.debezium.connector.postgresql.Source', 'field': 'source'}, {'type': 'string', 'optional': False, 'field': 'op'}, {'type': 'int64', 'optional': True, 'field': 'ts_ms'}, {'type': 'struct', 'fields': [{'type': 'string', 'optional': False, 'field': 'id'}, {'type': 'int64', 'optional': False, 'field': 'total_order'}, {'type': 'int64', 'optional': False, 'field': 'data_collection_order'}], 'optional': True, 'name': 'event.block', 'version': 1, 'field': 'transaction'}], 'optional': False, 'name': 'postgres.public.stores.Envelope', 'version': 1}, 'payload': {'before': None, 'after': {'store_id': 2, 'store_name': 'Store Y', 'store_country': 'Belarus', 'store_city': 'Minsk', 'store_address': '438 DARK SPURT'}, 'source': {'version': '2.2.0.Alpha3', 'connector': 'postgresql', 'name': 'postgres', 'ts_ms': 1702396316659, 'snapshot': 'false', 'db': 'postgres', 'sequence': '[\"50853736\",\"50854096\"]', 'schema': 'public', 'table': 'stores', 'txId': 754, 'lsn': 50854096, 'xmin': None}, 'op': 'c', 'ts_ms': 1702396317101, 'transaction': None}}\n",
      "+++\n",
      "insert into dwh_detailed.anc_sourceSystem (sourceSystemID) values ('e8a48653851e28c69d0506508fb27fc5') on conflict (sourceSystemID) do nothing;\n",
      "insert into dwh_detailed.attr_sourceSystem_name (sourceSystemID, name) values ('e8a48653851e28c69d0506508fb27fc5', 'postgres') on conflict (sourceSystemID) do nothing;\n",
      "insert into dwh_detailed.anc_store (storeId, sourceSystemID) values ('c81e728d9d4c2f636f067f89cc14862c', 'e8a48653851e28c69d0506508fb27fc5') on conflict (storeId) do nothing;\n",
      "insert into dwh_detailed.attr_store_businessKey (storeId, businessKey) values ('c81e728d9d4c2f636f067f89cc14862c', '2') on conflict (storeId) do nothing;\n",
      "insert into dwh_detailed.attr_store_name (storeId, storeName) values ('c81e728d9d4c2f636f067f89cc14862c', 'Store Y') on conflict (storeId) do nothing;\n",
      "insert into dwh_detailed.attr_store_address (storeId, storeAddress) values ('c81e728d9d4c2f636f067f89cc14862c', '438 DARK SPURT') on conflict (storeId) do nothing;\n",
      "insert into dwh_detailed.attr_store_city (storeId, storeCity) values ('c81e728d9d4c2f636f067f89cc14862c', 'Minsk') on conflict (storeId) do nothing;\n",
      "insert into dwh_detailed.attr_store_isActive (storeId, isActive) values ('c81e728d9d4c2f636f067f89cc14862c', true) on conflict (storeId) do nothing;\n",
      "insert into dwh_detailed.anc_country (countryId, sourceSystemID) values ('6542f875eaa09a5c550e5f3986400ad9', 'e8a48653851e28c69d0506508fb27fc5') on conflict (countryId) do nothing;\n",
      "insert into dwh_detailed.attr_country_name (countryId, countryName) values ('6542f875eaa09a5c550e5f3986400ad9', 'Belarus') on conflict (countryId) do nothing;\n",
      "insert into dwh_detailed.tie_store_country (storeId, countryId) values ('c81e728d9d4c2f636f067f89cc14862c', '6542f875eaa09a5c550e5f3986400ad9') on conflict (storeId, countryId) do nothing;\n",
      "\n",
      "---\n",
      "{'schema': {'type': 'struct', 'fields': [{'type': 'struct', 'fields': [{'type': 'int32', 'optional': False, 'default': 0, 'field': 'store_id'}, {'type': 'string', 'optional': False, 'field': 'store_name'}, {'type': 'string', 'optional': False, 'field': 'store_country'}, {'type': 'string', 'optional': False, 'field': 'store_city'}, {'type': 'string', 'optional': False, 'field': 'store_address'}], 'optional': True, 'name': 'postgres.public.stores.Value', 'field': 'before'}, {'type': 'struct', 'fields': [{'type': 'int32', 'optional': False, 'default': 0, 'field': 'store_id'}, {'type': 'string', 'optional': False, 'field': 'store_name'}, {'type': 'string', 'optional': False, 'field': 'store_country'}, {'type': 'string', 'optional': False, 'field': 'store_city'}, {'type': 'string', 'optional': False, 'field': 'store_address'}], 'optional': True, 'name': 'postgres.public.stores.Value', 'field': 'after'}, {'type': 'struct', 'fields': [{'type': 'string', 'optional': False, 'field': 'version'}, {'type': 'string', 'optional': False, 'field': 'connector'}, {'type': 'string', 'optional': False, 'field': 'name'}, {'type': 'int64', 'optional': False, 'field': 'ts_ms'}, {'type': 'string', 'optional': True, 'name': 'io.debezium.data.Enum', 'version': 1, 'parameters': {'allowed': 'true,last,false,incremental'}, 'default': 'false', 'field': 'snapshot'}, {'type': 'string', 'optional': False, 'field': 'db'}, {'type': 'string', 'optional': True, 'field': 'sequence'}, {'type': 'string', 'optional': False, 'field': 'schema'}, {'type': 'string', 'optional': False, 'field': 'table'}, {'type': 'int64', 'optional': True, 'field': 'txId'}, {'type': 'int64', 'optional': True, 'field': 'lsn'}, {'type': 'int64', 'optional': True, 'field': 'xmin'}], 'optional': False, 'name': 'io.debezium.connector.postgresql.Source', 'field': 'source'}, {'type': 'string', 'optional': False, 'field': 'op'}, {'type': 'int64', 'optional': True, 'field': 'ts_ms'}, {'type': 'struct', 'fields': [{'type': 'string', 'optional': False, 'field': 'id'}, {'type': 'int64', 'optional': False, 'field': 'total_order'}, {'type': 'int64', 'optional': False, 'field': 'data_collection_order'}], 'optional': True, 'name': 'event.block', 'version': 1, 'field': 'transaction'}], 'optional': False, 'name': 'postgres.public.stores.Envelope', 'version': 1}, 'payload': {'before': None, 'after': {'store_id': 3, 'store_name': 'Store Z', 'store_country': 'Ukraine', 'store_city': 'Kiev', 'store_address': 'ul. Kosmonavtov 35-11'}, 'source': {'version': '2.2.0.Alpha3', 'connector': 'postgresql', 'name': 'postgres', 'ts_ms': 1702396316659, 'snapshot': 'false', 'db': 'postgres', 'sequence': '[\"50853736\",\"50854256\"]', 'schema': 'public', 'table': 'stores', 'txId': 754, 'lsn': 50854256, 'xmin': None}, 'op': 'c', 'ts_ms': 1702396317102, 'transaction': None}}\n",
      "+++\n",
      "insert into dwh_detailed.anc_sourceSystem (sourceSystemID) values ('e8a48653851e28c69d0506508fb27fc5') on conflict (sourceSystemID) do nothing;\n",
      "insert into dwh_detailed.attr_sourceSystem_name (sourceSystemID, name) values ('e8a48653851e28c69d0506508fb27fc5', 'postgres') on conflict (sourceSystemID) do nothing;\n",
      "insert into dwh_detailed.anc_store (storeId, sourceSystemID) values ('eccbc87e4b5ce2fe28308fd9f2a7baf3', 'e8a48653851e28c69d0506508fb27fc5') on conflict (storeId) do nothing;\n",
      "insert into dwh_detailed.attr_store_businessKey (storeId, businessKey) values ('eccbc87e4b5ce2fe28308fd9f2a7baf3', '3') on conflict (storeId) do nothing;\n",
      "insert into dwh_detailed.attr_store_name (storeId, storeName) values ('eccbc87e4b5ce2fe28308fd9f2a7baf3', 'Store Z') on conflict (storeId) do nothing;\n",
      "insert into dwh_detailed.attr_store_address (storeId, storeAddress) values ('eccbc87e4b5ce2fe28308fd9f2a7baf3', 'ul. Kosmonavtov 35-11') on conflict (storeId) do nothing;\n",
      "insert into dwh_detailed.attr_store_city (storeId, storeCity) values ('eccbc87e4b5ce2fe28308fd9f2a7baf3', 'Kiev') on conflict (storeId) do nothing;\n",
      "insert into dwh_detailed.attr_store_isActive (storeId, isActive) values ('eccbc87e4b5ce2fe28308fd9f2a7baf3', true) on conflict (storeId) do nothing;\n",
      "insert into dwh_detailed.anc_country (countryId, sourceSystemID) values ('f01fc92b23faa973f3492a23d5a705c5', 'e8a48653851e28c69d0506508fb27fc5') on conflict (countryId) do nothing;\n",
      "insert into dwh_detailed.attr_country_name (countryId, countryName) values ('f01fc92b23faa973f3492a23d5a705c5', 'Ukraine') on conflict (countryId) do nothing;\n",
      "insert into dwh_detailed.tie_store_country (storeId, countryId) values ('eccbc87e4b5ce2fe28308fd9f2a7baf3', 'f01fc92b23faa973f3492a23d5a705c5') on conflict (storeId, countryId) do nothing;\n",
      "\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing consumer due to finish\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m StoreHandler(dwh_conn_string, kafka_server, kafka_consumer_group, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostgres.public.stores\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m a\u001b[38;5;241m.\u001b[39mconsume()\n",
      "Cell \u001b[0;32mIn[12], line 86\u001b[0m, in \u001b[0;36mStoreHandler.consume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconsume\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkafka_engine:\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;28mprint\u001b[39m(message\u001b[38;5;241m.\u001b[39mvalue)\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m+++\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/kafka/consumer/group.py:1193\u001b[0m, in \u001b[0;36mKafkaConsumer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v1()\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_v2()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/kafka/consumer/group.py:1201\u001b[0m, in \u001b[0;36mKafkaConsumer.next_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_generator_v2()\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/kafka/consumer/group.py:1116\u001b[0m, in \u001b[0;36mKafkaConsumer._message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_message_generator_v2\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1115\u001b[0m     timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consumer_timeout \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[0;32m-> 1116\u001b[0m     record_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39mtimeout_ms, update_offsets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tp, records \u001b[38;5;129;01min\u001b[39;00m six\u001b[38;5;241m.\u001b[39miteritems(record_map):\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;66;03m# Generators are stateful, and it is possible that the tp / records\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m         \u001b[38;5;66;03m# here may become stale during iteration -- i.e., we seek to a\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m         \u001b[38;5;66;03m# different offset, pause consumption, or lose assignment.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m records:\n\u001b[1;32m   1122\u001b[0m             \u001b[38;5;66;03m# is_fetchable(tp) should handle assignment changes and offset\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m             \u001b[38;5;66;03m# resets; for all other changes (e.g., seeks) we'll rely on the\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m             \u001b[38;5;66;03m# outer function destroying the existing iterator/generator\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m             \u001b[38;5;66;03m# via self._iterator = None\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/kafka/consumer/group.py:655\u001b[0m, in \u001b[0;36mKafkaConsumer.poll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    653\u001b[0m remaining \u001b[38;5;241m=\u001b[39m timeout_ms\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll_once(remaining, max_records, update_offsets\u001b[38;5;241m=\u001b[39mupdate_offsets)\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m records:\n\u001b[1;32m    657\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m records\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/kafka/consumer/group.py:702\u001b[0m, in \u001b[0;36mKafkaConsumer._poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    701\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout_ms, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mtime_to_next_poll() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m--> 702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpoll(timeout_ms\u001b[38;5;241m=\u001b[39mtimeout_ms)\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# after the long poll, we should check whether the group needs to rebalance\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;66;03m# prior to returning data so that the group can stabilize faster\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mneed_rejoin():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/kafka/client_async.py:602\u001b[0m, in \u001b[0;36mKafkaClient.poll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    599\u001b[0m             timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(timeout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretry_backoff_ms\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    600\u001b[0m         timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, timeout)  \u001b[38;5;66;03m# avoid negative timeouts\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# called without the lock to avoid deadlock potential\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# if handlers need to acquire locks\u001b[39;00m\n\u001b[1;32m    606\u001b[0m responses\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_pending_completed_requests())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/kafka/client_async.py:634\u001b[0m, in \u001b[0;36mKafkaClient._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_send_sockets()\n\u001b[1;32m    633\u001b[0m start_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 634\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mselect(timeout)\n\u001b[1;32m    635\u001b[0m end_select \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sensors:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/selectors.py:561\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    559\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 561\u001b[0m     kev_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mcontrol(\u001b[38;5;28;01mNone\u001b[39;00m, max_ev, timeout)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "a = StoreHandler(dwh_conn_string, kafka_server, kafka_consumer_group, 'postgres.public.stores')\n",
    "a.consume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57e4843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Добавление коннекторов Debezium. \"\"\"\n",
    "\n",
    "import requests\n",
    "from typing import Dict, Any\n",
    "\n",
    "DEBEZIUM_URL = 'http://localhost:8083/connectors'\n",
    "\n",
    "\n",
    "def add_connector(connector_dict: Dict[str, Any]) -> None:\n",
    "    response = requests.post(\n",
    "        DEBEZIUM_URL, \n",
    "        json=connector_dict\n",
    "    )\n",
    "    assert response.status_code < 400, response.text\n",
    "\n",
    "\n",
    "pg_connector_config = {\n",
    "  \"name\": \"pg-connector\",\n",
    "  \"config\": {\n",
    "    \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n",
    "    \"plugin.name\": \"pgoutput\",\n",
    "    \n",
    "    \"database.hostname\": \"host.docker.internal\",\n",
    "    \"database.port\": \"5432\",\n",
    "    \"database.user\": \"postgres\",\n",
    "    \"database.password\": \"postgres\",\n",
    "    \"database.dbname\" : \"postgres\",\n",
    "    \"database.server.name\": \"pg-dev\",\n",
    "    \n",
    "    \"table.include.list\": \"public.(.*)\",\n",
    "    \"heartbeat.interval.ms\": \"5000\",\n",
    "    \"slot.name\": \"dbname_debezium\",\n",
    "    \"publication.name\": \"dbname_publication\",\n",
    "      \n",
    "    \"topic.creation.default.cleanup.policy\": \"delete\",\n",
    "    \"topic.creation.default.partitions\": \"1\",\n",
    "    \"topic.creation.default.replication.factor\": \"1\",\n",
    "    \"topic.creation.default.retention.ms\": \"604800000\",\n",
    "    \"topic.creation.enable\": \"true\",\n",
    "    \"topic.prefix\": \"postgres\"\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "sink_connectors_info = {\n",
    "    'postgres.public.categories': 'category_id',\n",
    "    'postgres.public.customers': 'customer_id',\n",
    "    'postgres.public.manufacturers': 'manufacturer_id',\n",
    "    'postgres.public.deliveries': 'delivery_id',\n",
    "    'postgres.public.price_change': 'product_id,price_change_ts',\n",
    "    'postgres.public.products': 'product_id',\n",
    "    'postgres.public.purchase_items': 'product_id,purchase_id',\n",
    "    'postgres.public.purchases': 'purchase_id',\n",
    "    'postgres.public.stores': 'store_id'\n",
    "}\n",
    "\n",
    "\n",
    "def add_pg_connector() -> None:\n",
    "    add_connector(pg_connector_config)\n",
    "\n",
    "\n",
    "def add_sink_connectors() -> None:\n",
    "    for topic, record_keys in sink_connectors_info.items():\n",
    "        add_connector(\n",
    "            {\n",
    "            \"name\": topic.replace('.', '-'),\n",
    "            \"config\": {\n",
    "                \"connector.class\": \"io.debezium.connector.jdbc.JdbcSinkConnector\",\n",
    "                \"topics\": topic,\n",
    "                \"connection.url\": \"jdbc:postgresql://host.docker.internal:5434/postgres\",\n",
    "                \"connection.username\": \"postgres\",\n",
    "                \"connection.password\": \"postgres\",\n",
    "                \"tasks.max\":\"1\",\n",
    "                \"insert.mode\": \"upsert\",\n",
    "                \"delete.enabled\": \"false\",\n",
    "                \"primary.key.mode\": \"record_key\",\n",
    "                \"primary.key.fields\": record_keys,\n",
    "                \"schema.evolution\": \"basic\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "add_pg_connector()\n",
    "add_sink_connectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf2b3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
